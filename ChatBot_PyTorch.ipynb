{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot-PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+m3PYcK+u/lZAPLXKN9Ce",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahNasser98/ChatBot-PyTorch/blob/master/ChatBot_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ9OViA9aWsX",
        "outputId": "34670e9f-cf64-44dc-8075-1b81628d438a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlpaug'...\n",
            "remote: Enumerating objects: 5670, done.\u001b[K\n",
            "remote: Counting objects: 100% (1197/1197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (355/355), done.\u001b[K\n",
            "remote: Total 5670 (delta 853), reused 1144 (delta 833), pack-reused 4473\u001b[K\n",
            "Receiving objects: 100% (5670/5670), 3.27 MiB | 13.71 MiB/s, done.\n",
            "Resolving deltas: 100% (4015/4015), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/makcedward/nlpaug.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTmu4iAkd4Bp",
        "outputId": "96f91f74-00df-4cfa-c56c-3070f2925b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 46.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.50.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Collecting click==8.0\n",
            "  Downloading click-8.0.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.50-py3-none-any.whl size=895166 sha256=7a281c12f93422e59795fe3820f58ac7cd5a0d85432f1298083ec54f30b50cc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/72/54/519f0d5143cc6c73fa3297509123c86fc8586a7fdea8d25311\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed click-8.0.0 huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.50 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tsfo6HGjL0U",
        "outputId": "afa49e02-3411-4fa7-ec49-a04f7fdc5fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mnlpaug\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######Generating Dataset#######\n",
        "######Create Dataset###########\n",
        "%%writefile /content/nlpaug/example/textual_augmenter.py\n",
        "\n",
        "#import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "#import nlpaug.augmenter.sentence as nas\n",
        "#import nlpaug.flow as nafc\n",
        "import pandas as pd\n",
        "#from nlpaug.util import Action\n",
        "\n",
        "#aug = nas.ContextualWordEmbsForSentenceAug(model_path='gpt2')\n",
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', action=\"substitute\")\n",
        "\n",
        "\n",
        "intents = [\"greeting\", \"age\", \"date\", \"name\", \"goodbye\"]\n",
        "response_no = 20\n",
        "questions = ['hello!', 'how old are you?', 'what is the date?', 'what is your name?', 'See you later']\n",
        "\n",
        "data = pd.DataFrame(columns=['response', 'tag'])\n",
        "counter = 0\n",
        "\n",
        "for intent in range(len(intents)):\n",
        "  for row in range(response_no):\n",
        "    augmented_text = aug.augment(questions[intent])\n",
        "    data.loc[row+counter] = [augmented_text, intents[intent]]\n",
        "  counter = counter + 20\n",
        "data.to_csv('/content/sample_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ovhWEKsajcX",
        "outputId": "f6efb32f-da46-4ba9-feab-85e43d0f6304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/nlpaug/example/textual_augmenter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/nlpaug/example/textual_augmenter.py"
      ],
      "metadata": {
        "id": "eZHBxsZlbsV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45464954-1096-4da3-b07a-1cfbaef4b5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 25.0kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 500kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 684kB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 1.10MB/s]\n",
            "Downloading: 100% 420M/420M [00:07<00:00, 56.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# used a dictionary to represent an intents JSON file\n",
        "data = {\"intents\": [\n",
        "{\"tag\": \"greeting\",\n",
        " \"responses\": [\"Howdy Partner!\", \"Hello\", \"How are you doing?\",   \"Greetings!\", \"How do you do?\"]},\n",
        "{\"tag\": \"age\",\n",
        " \"responses\": [\"I am 25 years old\", \"I was born in 1998\", \"My birthday is July 3rd and I was born in 1998\", \"03/07/1998\"]},\n",
        "{\"tag\": \"date\",\n",
        " \"responses\": [\"I am available all week\", \"I don't have any plans\",  \"I am not busy\"]},\n",
        "{\"tag\": \"name\",\n",
        " \"responses\": [\"My name is James\", \"I'm James\", \"James\"]},\n",
        "{\"tag\": \"goodbye\",\n",
        " \"responses\": [\"It was nice speaking to you\", \"See you later\", \"Speak soon!\"]}\n",
        "]}"
      ],
      "metadata": {
        "id": "D2ialcQ2cIj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lS8zlH9WpIU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(r'/content/sample_data.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tlr6EdCyCgAn",
        "outputId": "b3606263-ba93-4771-87d1-249570af1854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           response       tag\n",
              "0             boom!  greeting\n",
              "1               ah!  greeting\n",
              "2             bang!  greeting\n",
              "3             boom!  greeting\n",
              "4              run!  greeting\n",
              "..              ...       ...\n",
              "95  see table later   goodbye\n",
              "96  see below later   goodbye\n",
              "97  see you tonight   goodbye\n",
              "98      see you now   goodbye\n",
              "99  catch you later   goodbye\n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0c9a64d-8b4d-4621-a846-eaceb7ffe9f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>boom!</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ah!</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bang!</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>boom!</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>run!</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>see table later</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>see below later</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>see you tonight</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>see you now</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>catch you later</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0c9a64d-8b4d-4621-a846-eaceb7ffe9f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f0c9a64d-8b4d-4621-a846-eaceb7ffe9f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f0c9a64d-8b4d-4621-a846-eaceb7ffe9f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############One hot encode labels######\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['tag'] = le.fit_transform(df['tag'])\n",
        "\n",
        "df['tag'].value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Un_TU9-Cvq1",
        "outputId": "dfd92665-8a26-485e-ce45-85a0737da8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    0.2\n",
              "0    0.2\n",
              "1    0.2\n",
              "4    0.2\n",
              "2    0.2\n",
              "Name: tag, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, train_labels = df['response'], df['tag']"
      ],
      "metadata": {
        "id": "T1m9zI7rRiJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######Start customizing the model\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# Import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShrRZvAuFT4q",
        "outputId": "2889537a-3477-439b-dd0c-64f37d2333aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######Test tokenizer######\n",
        "text = [\"this is a distil bert model.\",\"data is oil\"]\n",
        "\n",
        "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZnHvhoIFyHN",
        "outputId": "418f6469-bd21-4b40-c50d-97c512f25dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1037,  4487, 16643,  2140, 14324,  2944,  1012,\n",
            "           102],\n",
            "        [  101,  2951,  2003,  3514,   102,     0,     0,     0,     0,     0,\n",
            "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "mNotY_LqRMKV",
        "outputId": "94e839d3-96d9-4734-d542-944e3785db46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f582d81d3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the histogram we are selecting the max len as 4\n",
        "max_seq_len = 4"
      ],
      "metadata": {
        "id": "ufcZ27ALR-Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########Tokenize input######\n",
        "tokens_train = tokenizer(train_text.tolist(),\n",
        "                         max_length=max_seq_len,\n",
        "                         pad_to_max_length=True,\n",
        "                         truncation=True,\n",
        "                         return_token_type_ids=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du_eVMJLSQJ8",
        "outputId": "db9e5601-6c69-4145-9dce-e8e908139aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())"
      ],
      "metadata": {
        "id": "3J_GMyAkS2Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#define a batch size\n",
        "batch_size = 16\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "# DataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "f_TCxQmVTGtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "   def __init__(self, bert):      \n",
        "       super(BERT_Arch, self).__init__()\n",
        "       self.bert = bert \n",
        "      \n",
        "       # dropout layer\n",
        "       self.dropout = nn.Dropout(0.2)\n",
        "      \n",
        "       # relu activation function\n",
        "       self.relu =  nn.ReLU()\n",
        "       # dense layer\n",
        "       self.fc1 = nn.Linear(768,512)\n",
        "       self.fc2 = nn.Linear(512,256)\n",
        "       self.fc3 = nn.Linear(256,5)\n",
        "       #softmax activation function\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "       #define the forward pass\n",
        "   def forward(self, sent_id, mask):\n",
        "      #pass the inputs to the model  \n",
        "      cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      \n",
        "      x = self.fc2(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      # output layer\n",
        "      x = self.fc3(x)\n",
        "   \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "Zr2a42M6T-O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get model summary\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT6KYgn3UHg0",
        "outputId": "af608df1-fbbe-4d7d-e1fd-68cbc954d356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#freeze all the parameters. This will prevent updating of model weights during fine-tuning.\n",
        "for param in bert.parameters():\n",
        "      param.requires_grad = False\n",
        "model = BERT_Arch(bert)\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru8dMKdSUAQb",
        "outputId": "f07f486a-761c-4b86-8daf-3cc328fd0c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================\n",
              "Layer (type:depth-idx)                                  Param #\n",
              "================================================================================\n",
              "BERT_Arch                                               --\n",
              "├─BertModel: 1-1                                        --\n",
              "│    └─BertEmbeddings: 2-1                              --\n",
              "│    │    └─Embedding: 3-1                              (23,440,896)\n",
              "│    │    └─Embedding: 3-2                              (393,216)\n",
              "│    │    └─Embedding: 3-3                              (1,536)\n",
              "│    │    └─LayerNorm: 3-4                              (1,536)\n",
              "│    │    └─Dropout: 3-5                                --\n",
              "│    └─BertEncoder: 2-2                                 --\n",
              "│    │    └─ModuleList: 3-6                             (85,054,464)\n",
              "│    └─BertPooler: 2-3                                  --\n",
              "│    │    └─Linear: 3-7                                 (590,592)\n",
              "│    │    └─Tanh: 3-8                                   --\n",
              "├─Dropout: 1-2                                          --\n",
              "├─ReLU: 1-3                                             --\n",
              "├─Linear: 1-4                                           393,728\n",
              "├─Linear: 1-5                                           131,328\n",
              "├─Linear: 1-6                                           1,285\n",
              "├─LogSoftmax: 1-7                                       --\n",
              "================================================================================\n",
              "Total params: 110,008,581\n",
              "Trainable params: 526,341\n",
              "Non-trainable params: 109,482,240\n",
              "================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MYJOwpIUJO0",
        "outputId": "a367d920-c373-4f98-a5ed-991fe178cc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# #compute the class weights\n",
        "# class_wts = compute_class_weight(‘balanced’, np.unique(train_labels), train_labels)\n",
        "# print(class_wts)\n",
        "# # convert class weights to tensor\n",
        "# weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "# weights = weights.to(device)\n",
        "# # loss function\n",
        "# cross_entropy = nn.NLLLoss(weight=weights)"
      ],
      "metadata": {
        "id": "EUZYhveOUo7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# #compute the class weights\n",
        "# class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "# print(class_wts)\n",
        "# # convert class weights to tensor\n",
        "\n",
        "# weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "# weights = weights.to(device)\n",
        "# loss function\n",
        "cross_entropy = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "QZq2xRm3UvQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler \n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "training_acc = []\n",
        "# number of training epochs\n",
        "epochs = 50\n",
        "# We can also use learning rate scheduler to achieve better results\n",
        "lr_sch = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
      ],
      "metadata": {
        "id": "fUi_dqOLU2Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  avg_loss = 0\n",
        "\n",
        "  correct = 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds = 0\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    #if step % 50 == 0 and not step == 0:\n",
        "      #print('  Batch {:>5,}  of  {:>5,}.'.format(step,    len(train_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch] \n",
        "    sent_id, mask, labels = batch\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    \n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    #calculate acc\n",
        "    _, predicted = torch.max(preds, 1)\n",
        "    correct += (predicted == labels).float().sum()\n",
        "\n",
        "    \n",
        "    # clear calculated gradients\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # We are not using learning rate scheduler as of now\n",
        "    # lr_sch.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    #total_preds.append(preds)\n",
        "    total_preds += len(labels)\n",
        "    # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  accuracy = 100 * correct / total_preds\n",
        "    #print(accuracy)\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "Po-4Ot-sU4lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, acc = train()\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "#print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    #print('Training Loss: ', train_loss)\n",
        "    print('Training_loss: {} / Training Accuracy: {}'.format(train_loss, acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySAkQ4pBR6yb",
        "outputId": "4d666238-d456-4437-b7e0-6dd84f052035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 50\n",
            "Training_loss: 0.10228875726949939 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 2 / 50\n",
            "Training_loss: 0.1360980167103532 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 3 / 50\n",
            "Training_loss: 0.10558888635465051 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 4 / 50\n",
            "Training_loss: 0.12868462236864225 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 5 / 50\n",
            "Training_loss: 0.09340531458396331 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 6 / 50\n",
            "Training_loss: 0.09021519473909782 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 7 / 50\n",
            "Training_loss: 0.09359480465562749 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 8 / 50\n",
            "Training_loss: 0.08584428312523025 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 9 / 50\n",
            "Training_loss: 0.10470935359314483 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 10 / 50\n",
            "Training_loss: 0.1080637297460011 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 11 / 50\n",
            "Training_loss: 0.10692728230994979 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 12 / 50\n",
            "Training_loss: 0.12276852556637355 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 13 / 50\n",
            "Training_loss: 0.0750232654333526 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 14 / 50\n",
            "Training_loss: 0.09204956410185007 / Training Accuracy: 92.0\n",
            "\n",
            " Epoch 15 / 50\n",
            "Training_loss: 0.08746399996536118 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 16 / 50\n",
            "Training_loss: 0.07351334138106072 / Training Accuracy: 97.0\n",
            "\n",
            " Epoch 17 / 50\n",
            "Training_loss: 0.1924444232136011 / Training Accuracy: 92.0\n",
            "\n",
            " Epoch 18 / 50\n",
            "Training_loss: 0.09622028143660698 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 19 / 50\n",
            "Training_loss: 0.1235651123736586 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 20 / 50\n",
            "Training_loss: 0.14911175891757011 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 21 / 50\n",
            "Training_loss: 0.12308897289247918 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 22 / 50\n",
            "Training_loss: 0.13453225860679854 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 23 / 50\n",
            "Training_loss: 0.15119875729526808 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 24 / 50\n",
            "Training_loss: 0.1200232423309769 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 25 / 50\n",
            "Training_loss: 0.09627229933227811 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 26 / 50\n",
            "Training_loss: 0.07657205538738968 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 27 / 50\n",
            "Training_loss: 0.0927016722738959 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 28 / 50\n",
            "Training_loss: 0.09789452532017354 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 29 / 50\n",
            "Training_loss: 0.08278788519757033 / Training Accuracy: 96.0\n",
            "\n",
            " Epoch 30 / 50\n",
            "Training_loss: 0.1262419016898743 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 31 / 50\n",
            "Training_loss: 0.08480620374523694 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 32 / 50\n",
            "Training_loss: 0.08927701017819142 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 33 / 50\n",
            "Training_loss: 0.17691524805767195 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 34 / 50\n",
            "Training_loss: 0.09473412670195103 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 35 / 50\n",
            "Training_loss: 0.21269731303410871 / Training Accuracy: 92.0\n",
            "\n",
            " Epoch 36 / 50\n",
            "Training_loss: 0.16035306506923266 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 37 / 50\n",
            "Training_loss: 0.24292466044425964 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 38 / 50\n",
            "Training_loss: 0.08562456495461188 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 39 / 50\n",
            "Training_loss: 0.08678484397894083 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 40 / 50\n",
            "Training_loss: 0.10317022832376617 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 41 / 50\n",
            "Training_loss: 0.14705438007201468 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 42 / 50\n",
            "Training_loss: 0.11553214770226207 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 43 / 50\n",
            "Training_loss: 0.22340952531833733 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 44 / 50\n",
            "Training_loss: 0.0841347015063675 / Training Accuracy: 93.0\n",
            "\n",
            " Epoch 45 / 50\n",
            "Training_loss: 0.08631243343855236 / Training Accuracy: 97.0\n",
            "\n",
            " Epoch 46 / 50\n",
            "Training_loss: 0.11601162648626737 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 47 / 50\n",
            "Training_loss: 0.09427565263052072 / Training Accuracy: 97.0\n",
            "\n",
            " Epoch 48 / 50\n",
            "Training_loss: 0.08961701348484244 / Training Accuracy: 95.0\n",
            "\n",
            " Epoch 49 / 50\n",
            "Training_loss: 0.09808526955125021 / Training Accuracy: 94.0\n",
            "\n",
            " Epoch 50 / 50\n",
            "Training_loss: 0.1712867602473125 / Training Accuracy: 92.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(str):\n",
        " str = re.sub(r'[^a-zA-Z ]+', '', str)\n",
        " test_text = [str]\n",
        " model.eval()\n",
        " \n",
        " tokens_test_data = tokenizer(\n",
        " test_text,\n",
        " max_length = max_seq_len,\n",
        " pad_to_max_length=True,\n",
        " truncation=True,\n",
        " return_token_type_ids=False\n",
        " )\n",
        " test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
        " test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
        " \n",
        " preds = None\n",
        " with torch.no_grad():\n",
        "   preds = model(test_seq.to(device), test_mask.to(device))\n",
        " preds = preds.detach().cpu().numpy()\n",
        " preds = np.argmax(preds, axis = 1)\n",
        " print(\"Intent Identified: \", le.inverse_transform(preds)[0])\n",
        " return le.inverse_transform(preds)[0]\n",
        "def get_response(message): \n",
        "  intent = get_prediction(message)\n",
        "  for i in data['intents']: \n",
        "    if i[\"tag\"] == intent:\n",
        "      result = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  print(f\"Response : {result}\")\n",
        "  return \"Intent: \"+ intent + '\\n' + \"Response: \" + result"
      ],
      "metadata": {
        "id": "_VS8ORKUSBOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_response(\"see you later\")"
      ],
      "metadata": {
        "id": "ZAIMFQfiSCIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "dedca193-dfc3-4849-fc60-e21c0d122d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent Identified:  goodbye\n",
            "Response : Speak soon!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Intent: goodbye/nResponse: Speak soon!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zmGJRjFQSVXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}